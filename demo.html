<!DOCTYPE html>
<!-- saved from url=(0040)http://www.vision.huji.ac.il/vid2speech/ -->
<html lang="en" class="gr__vision_huji_ac_il"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Chinese Speech Reconstruction from Silent Video</title>

  <!-- CSS includes -->
  <link href="./files/bootstrap.min.css" rel="stylesheet">
  <link href="./files/css" rel="stylesheet" type="text/css">
  <link href="./files/mystyle.css" rel="stylesheet">
  <meta name="citation_title" content="Vid2speech: Speech Reconstruction from Silent Video">
  <meta name="citation_author" content="Huaxuan, GAO">
  <meta name="citation_author" content="Cheng, LIU">
  <meta name="citation_date" content="2019/03/29">
  <meta name="citation_online_date" content="2019/03/29">

  <script async="" src="./files/analytics.js"></script><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
   (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
   m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
 })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

 ga('create', 'UA-70806372-2', 'auto');
 ga('send', 'pageview');
 </script>

</head>
<body data-gr-c-s-loaded="true">

  <!-- Start of StatCounter Code for Default Guide -->
  <script type="text/javascript">
  var sc_project=11210212;
  var sc_invisible=1;
  var sc_security="daedde79";
  var scJsHost = (("https:" == document.location.protocol) ?
    "https://secure." : "http://www.");
  document.write("<sc"+"ript type='text/javascript' src='" + scJsHost+
    "statcounter.com/counter/counter.js'></"+"script>");
  </script><script type="text/javascript" src="./files/counter.js"></script>
  <noscript><div class="statcounter"><a title="web statistics"
    href="http://statcounter.com/" target="_blank"><img class="statcounter"
    src="//c.statcounter.com/11210212/0/daedde79/1/" alt="web
    statistics"></a></div></noscript>
    <!-- End of StatCounter Code for Default Guide -->

    <div id="header" class="container-fluid">
      <div class="row">
        <h1>Chinese Speech Reconstruction from Silent Video</h1>
        <div class="authors">
          <a href="https://github.com/HuaxuanGAO">Huaxuan Gao</a> &nbsp;&nbsp;&nbsp;
          <a href="https://github.com/LIUChengAndr">Cheng LIU</a> &nbsp;&nbsp;&nbsp;
        </div>
      </div>
    </div>
    <div class="container-fluid">
      <div class="row" style="text-align: center;">
        <div class="logoimg">
          <a href="https://www.ust.hk/">
            <img src="./files/HKUST_logo.png" height="95">
          </a>
        </div>
      </div>

      <div class="container" id="abstractdiv">
        <h2>Introduction</h2>
		<br>
    This project explores reconstructing audio from a silent video in Chinese and aims at filling two gaps. For the lip reading problem, works have been done for video to text, text to audio, but not much has been done to construct audios directly from videos. Moreover, existing works focus mainly on English, whereas our work is targeting at Chinese. 

        <br>
        In this project, we implemented a convolutional neural network, that takes silent video as input, and outputs the LPC encoding of the reconstructed audio, which can be transform into wave form. A new dataset of Chinese speaker is collected, and we showed that the model trained on this dataset can produce intelligible speech. The reconstructed audio can significantly improve the understanding of the video content for human listeners, compared to silent videos.
        <br> This website provides a demo program to help you understand our project. 
      </div>

      <div class="container" id="paperdiv">
        <h2>VID2SPEECH: SPEECH RECONSTRUCTION FROM SILENT VIDEO</h2>
        <a href="https://arxiv.org/pdf/1701.00495.pdf">
          <div class="thumbs">
            <img src="./files/vid2speech1.png" width="20%">
            <img src="./files/vid2speech2.png" width="39%">
            <img src="./files/vid2speech3.png" width="39%">
          </div>
        </a>
        <h2>Supplementary Materials</h2>
		<h3>Video example</h3>
        The following video contains a few examples of silent video frames given to our model as input, along with the acoustic signal generated by the system. The example shown below is a speaker from a <a href="http://spandh.dcs.shef.ac.uk/gridcorpus/">Chinese corpus</a> constructed according to the <a href="http://spandh.dcs.shef.ac.uk/gridcorpus/">GRID corpus</a>. Separate models were trained on % of the data from each speaker, and the training set contained all words present in the testing set.
        <br>
       
        <br>
        <br>
        <video width="640" height="320" controls><source src="./files/sample.mp4" type="video/mp4"></video>
        <br>
		<h3>Corpus for Reference</h3>
		This part shoule be a table of the corpus of our Chinese dataset
		<table style="width:100%">
			<caption>Corpus</caption>
			<tr>
				<th>w</th>
				<th>w</th>
			</tr>
			<tr>
				<td>w</td>
				<td>w</td>
			</tr>
			<tr>
				<td>w</td>
				<td>w</td>
			</tr>
		</table>
        <div>
          <h4>BibTeX</h4>
          <pre class="citation">@article{ephrat2017improved,
            title={Improved Speech Reconstruction from Silent Video},
            author={Ephrat, Ariel and Halperin, Tavi and Peleg, Shmuel},
            journal={ICCV 2017 Workshop on Computer Vision for Audio-Visual Media},
            year={2017}
          }</pre>
        </div>
        <br>
        <h4>Code and Paper</h4>
        Code for this work is being prepared for release. In the meantime, code for our  paper can be found <a href="https://github.com/arielephrat/vid2speech">here.</a> Enjoy!
      </div>

      <div class="container" id="paperdiv">
        <h2>To be modified</h2>
        <a href="http://www.cs.huji.ac.il/~arielephrat/papers/vid2speech_icassp17.pdf">
          <div class="thumbs">
            <img src="./files/thumb_icassp17_1.png" width="19%">
            <img src="./files/thumb_icassp17_2.png" width="19%">
            <img src="./files/thumb_icassp17_3.png" width="19%">
            <img src="./files/thumb_icassp17_4.png" width="19%">
            <img src="./files/thumb_icassp17_5.png" width="19%">
          </div>
        </a>
        <h2>Samples of Reconstructed Speech</h2>
        The following videos consist of original video frames given to our CNN as input, along with the acoustic signal generated by the system. We performed a human listening test using Amazon Mechanical Turk workers in order to evaluate the reconstructed speech intelligibility. Presented here are two examples from each transcription task given to MTurk workers.
        <h4>Reconstruction from full dataset</h4>
        For this task we trained our model on a random 80/20 train/test split of the 1000 videos of the speaker from the <a href="http://spandh.dcs.shef.ac.uk/gridcorpus/">Chinese corpus</a>, and made sure that all words in the corpus were represented in each set.
        <br>
        <h4>S2</h4>
        <video width="373" height="210" controls><source src="./files/sample.mp4" type="video/mp4"></video>
        <video width="373" height="210" controls><source src="./files/sample.mp4" type="video/mp4"></video>
        <h4>S4</h4>
        <video width="373" height="210" controls><source src="./files/sample.mp4" type="video/mp4"></video>
        <video width="373" height="210" controls><source src="./files/sample.mp4" type="video/mp4"></video>
        <h4>Reconstructing out-of-vocabulary (OOV) words</h4>
        In this task, the videos in our dataset were sorted according to the digit (fifth word) uttered in each sentence, and our network was trained and tested on five different train/test splits - each with two distinct digits left out of the training set. For example, for the following video on the left side, the network was trained on all videos with the numbers 3,4,..,9,0 uttered, and tested only on the ones containing the numbers 1 and 2.
        <br>
        <br>
        <video width="373" height="210" controls><source src="./files/sample.mp4" type="video/mp4"></video>
        <video width="373" height="210" controls><source src="./files/sample.mp4" type="video/mp4"></video>
        <br>
        For the left video, correct digit is "2", and the right is "7".
        <br>
        <br>
        <div>
          <h4>BibTeX</h4>
          <pre class="citation">@inproceedings{ephrat2017vid2speech,
            title     = {Vid2Speech: speech reconstruction from silent video},
            author    = {Ariel Ephrat and Shmuel Peleg},
            booktitle = {2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
            year      = {2017},
            organization = {IEEE}
          }</pre>
        </div>
        <br>
        <h4>Code</h4>
        Code for this work can be found <a href="https://github.com/HuaxuanGAO">here.</a> Enjoy!
        <div>
        <h2>Try it yourself here, upload a silent video an reconstructed autio would be generated.</h2>
        <input type="file" id="myFile">


    <input type="button" id='script' name="scriptbutton" value=" Run Script " onclick="goPython()">

    <script src="http://code.jquery.com/jquery-3.3.1.min.js" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script>

    <script>
        function goPython(){
            $.ajax({
              url: "MYSCRIPT.py",
             context: document.body
            }).done(function() {
             alert('finished python script');;
            });
        }
    </script>
        <h4>The video with original audio(left) VS The video with reconstructed audio(right): </h4>
        <video width="45%" controls><source src="./files/original.mp4" value = "original" name = "original" type="video/mp4"></video>
        <video width="45%" controls><source src="./files/original.mp4" value = "reconstructed" name = "reconstructed" type="video/mp4"></video>
        <h4>The spectrogram of the original audio(left) VS the spectrogram of the recongstructed audio(right): </h4>

		<img width="45%"  src="./files/origin_mel.png" value = "original" name = "original" >
		<img width="45%"  src="./files/rec_mel.png" value = "original" name = "original" >

		<h4>Corpus for Reference</h4>
		The table below illustrates the corpus of the demo program.
		<table style="width:100%">
			<caption>Corpus</caption>
			<tr>
				<th>w</th>
				<th>w</th>
			</tr>
			<tr>
				<td>w</td>
				<td>w</td>
			</tr>
			<tr>
				<td>w</td>
				<td>w</td>
			</tr>
		</table>
    <script>
        function goPython(){
            $.ajax({
              url: "audioReconstruction.py",
             context: document.body
            }).done(function() {
             alert('finished python script');;
            });
        }
    </script>
      </div>

      <div>
		    <div>
      <h2>Related papers</h2>
      <ul>
        <li><a href="http://www.vision.huji.ac.il/speaker-separation">Seeing Through Noise: Visually Driven Speaker Separation and Enhancement</a></li>
        <li><a href="http://www.vision.huji.ac.il/visual-speech-enhancement">Visual Speech Enhancement</a></li>
      </ul>
    </div>
        <div id="footer">
        </div>

      
      

</div></div></body><span class="gr__tooltip"><span class="gr__tooltip-content"></span><i class="gr__tooltip-logo"></i><span class="gr__triangle"></span></span></html>
