<!DOCTYPE html>
<!-- saved from url=(0040)http://www.vision.huji.ac.il/vid2speech/ -->
<html lang="en" class="gr__vision_huji_ac_il"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Chinese Speech Reconstruction from Silent Video</title>

  <!-- CSS includes -->
  <link href="./files/bootstrap.min.css" rel="stylesheet">
  <link href="./files/css" rel="stylesheet" type="text/css">
  <link href="./files/mystyle.css" rel="stylesheet">
  <meta name="citation_title" content="Vid2speech: Speech Reconstruction from Silent Video">
  <meta name="citation_author" content="Huaxuan, GAO">
  <meta name="citation_author" content="Cheng, LIU">
  <meta name="citation_date" content="2019/03/29">
  <meta name="citation_online_date" content="2019/03/29">

  <script async="" src="./files/analytics.js"></script><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
   (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
   m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
 })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

 ga('create', 'UA-70806372-2', 'auto');
 ga('send', 'pageview');
 </script>

</head>
<body data-gr-c-s-loaded="true">

  <!-- Start of StatCounter Code for Default Guide -->
  <script type="text/javascript">
  var sc_project=11210212;
  var sc_invisible=1;
  var sc_security="daedde79";
  var scJsHost = (("https:" == document.location.protocol) ?
    "https://secure." : "http://www.");
  document.write("<sc"+"ript type='text/javascript' src='" + scJsHost+
    "statcounter.com/counter/counter.js'></"+"script>");
  </script><script type="text/javascript" src="./files/counter.js"></script>
  <noscript><div class="statcounter"><a title="web statistics"
    href="http://statcounter.com/" target="_blank"><img class="statcounter"
    src="//c.statcounter.com/11210212/0/daedde79/1/" alt="web
    statistics"></a></div></noscript>
    <!-- End of StatCounter Code for Default Guide -->

    <div id="header" class="container-fluid">
      <div class="row">
        <h1>Chinese Speech Reconstruction from Silent Video</h1>
        <div class="authors">
          <a href="https://github.com/HuaxuanGAO">Huaxuan Gao</a> &nbsp;&nbsp;&nbsp;
          <a href="https://github.com/LIUChengAndr">Cheng LIU</a> &nbsp;&nbsp;&nbsp;
        </div>
      </div>
    </div>
    <div class="container-fluid">
      <div class="row" style="text-align: center;">
        <div class="logoimg">
          <a href="https://www.ust.hk/">
            <img src="./files/HKUST_logo.png" height="95">
          </a>
        </div>
      </div>

      <div class="container" id="abstractdiv">
        <h2>Abstract(To be modified)</h2>
        This project deals with an important aspect of the lip reading problem: speech reconstruction from silent video. Lipreading, also known as speechreading, is a technique to use only visual information on a human face, such as the movement of the lips and perhaps other muscles on the face, to understand the information conveyed from the speaker [1].
        <br>
		In the task of speechreading, there are two basic units to consider, phonemes and visemes. A phoneme is the smallest detectable unit of sound in a language that serves to distinguish words from one another. A visemes is a visually distinctive unit, which is normally smaller than a phoneme, because many phonemes are produced within the mouth and throat, and cannot be seen, which usually results in several phonemes mapping to one or more visemes.
		<br>
		The major difficulty for speechreading comes from the many-to-one mapping between phonemes and visemes, because the statistical distribution of phonemes to different viseme groups is not uniform. This means that some visemes might correspond to fewer phonemes, which decreases the ambiguity and thus allows us to distinguish the phonemes more easily. Furthermore, with external information, such as the context of the conversation and the emotion of the speaker, we can make more precise judgements.
		<br>
		In this project, a machine learning model is implemented and trained to reconstruct audio messages from silent videos. In addition, a Chinese video dataset will be collected for training the model.

        <br>
        We train our model on speakers from the GRID and TCD-TIMIT datasets, and evaluate the quality and intelligibility of reconstructed speech using common objective measurements. We show that speech predictions from the proposed model attain scores which indicate significantly improved quality over existing models. In addition, we show promising results towards reconstructing speech from an unconstrained dictionary.
      </div>

      <div class="container" id="paperdiv">
        <h2>VID2SPEECH: SPEECH RECONSTRUCTION FROM SILENT VIDEO(To be modified)</h2>
        <a href="https://arxiv.org/pdf/1701.00495.pdf">
          <div class="thumbs">
            <img src="./files/vid2speech1.png" width="20%">
            <img src="./files/vid2speech2.png" width="39%">
            <img src="./files/vid2speech3.png" width="39%">
          </div>
        </a>
        <h2>Supplementary Materials</h2>
		<h3>Video example</h3>
        The following video contains a few examples of silent video frames given to our model as input, along with the acoustic signal generated by the system. The example shown below is a speaker from a <a href="http://spandh.dcs.shef.ac.uk/gridcorpus/">Chinese corpus</a> constructed according to the <a href="http://spandh.dcs.shef.ac.uk/gridcorpus/">GRID corpus</a>. Separate models were trained on % of the data from each speaker, and the training set contained all words present in the testing set.
        <br>
       
        <br>
        <br>
        <video width="640" height="320" controls><source src="./files/sample.mp4" type="video/mp4"></video>
        <br>
		<h3>Corpus for Reference</h3>
		This part shoule be a table of the corpus of our Chinese dataset
		<table style="width:100%">
			<caption>Corpus</caption>
			<tr>
				<th>w</th>
				<th>w</th>
			</tr>
			<tr>
				<td>w</td>
				<td>w</td>
			</tr>
			<tr>
				<td>w</td>
				<td>w</td>
			</tr>
		</table>
        <div>
          <h4>BibTeX</h4>
          <pre class="citation">@article{ephrat2017improved,
            title={Improved Speech Reconstruction from Silent Video},
            author={Ephrat, Ariel and Halperin, Tavi and Peleg, Shmuel},
            journal={ICCV 2017 Workshop on Computer Vision for Audio-Visual Media},
            year={2017}
          }</pre>
        </div>
        <br>
        <h4>Code and Paper</h4>
        Code for this work is being prepared for release. In the meantime, code for our  paper can be found <a href="https://github.com/arielephrat/vid2speech">here.</a> Enjoy!
      </div>

      <div class="container" id="paperdiv">
        <h2>To be modified</h2>
        <a href="http://www.cs.huji.ac.il/~arielephrat/papers/vid2speech_icassp17.pdf">
          <div class="thumbs">
            <img src="./files/thumb_icassp17_1.png" width="19%">
            <img src="./files/thumb_icassp17_2.png" width="19%">
            <img src="./files/thumb_icassp17_3.png" width="19%">
            <img src="./files/thumb_icassp17_4.png" width="19%">
            <img src="./files/thumb_icassp17_5.png" width="19%">
          </div>
        </a>
        <h2>Samples of Reconstructed Speech</h2>
        The following videos consist of original video frames given to our CNN as input, along with the acoustic signal generated by the system. We performed a human listening test using Amazon Mechanical Turk workers in order to evaluate the reconstructed speech intelligibility. Presented here are two examples from each transcription task given to MTurk workers.
        <h4>Reconstruction from full dataset</h4>
        For this task we trained our model on a random 80/20 train/test split of the 1000 videos of the speaker from the <a href="http://spandh.dcs.shef.ac.uk/gridcorpus/">Chinese corpus</a>, and made sure that all words in the corpus were represented in each set.
        <br>
        <h4>S2</h4>
        <video width="373" height="210" controls><source src="./files/sample.mp4" type="video/mp4"></video>
        <video width="373" height="210" controls><source src="./files/sample.mp4" type="video/mp4"></video>
        <h4>S4</h4>
        <video width="373" height="210" controls><source src="./files/sample.mp4" type="video/mp4"></video>
        <video width="373" height="210" controls><source src="./files/sample.mp4" type="video/mp4"></video>
        <h4>Reconstructing out-of-vocabulary (OOV) words</h4>
        In this task, the videos in our dataset were sorted according to the digit (fifth word) uttered in each sentence, and our network was trained and tested on five different train/test splits - each with two distinct digits left out of the training set. For example, for the following video on the left side, the network was trained on all videos with the numbers 3,4,..,9,0 uttered, and tested only on the ones containing the numbers 1 and 2.
        <br>
        <br>
        <video width="373" height="210" controls><source src="./files/sample.mp4" type="video/mp4"></video>
        <video width="373" height="210" controls><source src="./files/sample.mp4" type="video/mp4"></video>
        <br>
        For the left video, correct digit is "2", and the right is "7".
        <br>
        <br>
        <div>
          <h4>BibTeX</h4>
          <pre class="citation">@inproceedings{ephrat2017vid2speech,
            title     = {Vid2Speech: speech reconstruction from silent video},
            author    = {Ariel Ephrat and Shmuel Peleg},
            booktitle = {2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
            year      = {2017},
            organization = {IEEE}
          }</pre>
        </div>
        <br>
        <h4>Code</h4>
        Code for this work can be found <a href="https://github.com/HuaxuanGAO">here.</a> Enjoy!
        <div>
        <h2>Try it yourself here, upload a silent video an reconstructed autio would be generated.</h2>
        <input type="file" id="myFile">
        <button onclick="myFunction()">reconstruct the audio</button>
        <script>
          function myFunction() {
            alert('debug1');
            exec('echo');
            alert('debug2');
            var x = document.createElement("body");
            alert('debug3');
            var y = document.getElementsById("myFile");
            alert('debug4');
            x.setAttribute("value", y.value);
            alert('debug5');
            document.body.appendChild(x);
            alert('debug6');
          }
        </script>
        </div>
      </div>

      <div>
		    <div>
      <h2>Related papers</h2>
      <ul>
        <li><a href="http://www.vision.huji.ac.il/speaker-separation">Seeing Through Noise: Visually Driven Speaker Separation and Enhancement</a></li>
        <li><a href="http://www.vision.huji.ac.il/visual-speech-enhancement">Visual Speech Enhancement</a></li>
      </ul>
    </div>
        <div id="footer">
        </div>

      
      

</div></div></body><span class="gr__tooltip"><span class="gr__tooltip-content"></span><i class="gr__tooltip-logo"></i><span class="gr__triangle"></span></span></html>
